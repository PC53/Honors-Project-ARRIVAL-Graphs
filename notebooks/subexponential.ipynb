{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from arrival_networkx.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from arrival_networkx import *\n",
    "import numpy as np\n",
    "import random as rm\n",
    "from typing import List\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_phi(number_of_nodes):\n",
    "    return np.sqrt(3) / np.sqrt(2*number_of_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_run_procedure(instance: Arrival, S_subset: List[int],w : dict):\n",
    "       \n",
    "    t = {s:0 for s in instance.vertices}\n",
    "    \n",
    "    t[0] = 1     # t[o] â† 1 /* traversal of (Y, o) */\n",
    "    for v in S_subset:\n",
    "        t[instance.s_0[v]] += np.ceil(w[v]/2) # /* dwi/2e traversals of (vi, seven(vi)) */\n",
    "        t[instance.s_1[v]] += np.floor(w[v]/2) # /* bwi/2c traversals of (vi, sodd(vi)) */\n",
    "    \n",
    "\n",
    "    # Let scurr and snext be arrays indexed by the vertices of V \\ S\n",
    "    s_curr =  instance.s_0 \n",
    "    s_next = instance.s_1\n",
    "            \n",
    "            \n",
    "    waiting_set = []\n",
    "    for v in instance.vertices:\n",
    "        if v not in S_subset and v not in [instance.target_node, instance.sink_node]:\n",
    "            waiting_set.append(v)\n",
    "            \n",
    "    while len(waiting_set) > 0: ## maintain all the vertices not in S \n",
    "        waiting_set_ = [ws for ws in waiting_set if t[ws]>0]\n",
    "        if not waiting_set_:\n",
    "            break\n",
    "        \n",
    "        choose = rm.choice(waiting_set_)\n",
    "        tau = rm.randint(1,t[choose])\n",
    "        \n",
    "        t[choose] -= tau\n",
    "        t[s_curr[choose]] += np.ceil(tau/2)\n",
    "        t[s_next[choose]] += np.floor(tau/2)\n",
    "        \n",
    "        if tau & 1 : \n",
    "            temp = s_curr[choose]\n",
    "            s_curr[choose] = s_next[choose]\n",
    "            s_next[choose] = temp\n",
    "    # print(t)      \n",
    "    return t[instance.target_node], t[instance.sink_node], {v:t[v] for v in S_subset}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_into_layers(instance: Arrival):\n",
    "    \"\"\"\n",
    "    Decompose the graph into layers based on the distance of the vertices to the destination nodes.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX graph instance.\n",
    "    - target_node: The target node (d).\n",
    "    - sink_node: The sink node (d').\n",
    "    \n",
    "    Returns:\n",
    "    - layers: A dictionary where key is the layer index and value is a list of nodes in that layer.\n",
    "    - max_dist: The maximum distance (layer index) found.\n",
    "    \"\"\"\n",
    "    graph = instance.graph    \n",
    "    # Initialize the layers dictionary\n",
    "    layers = {}\n",
    "    # Distance dictionary with initial values set to None for each node\n",
    "    dist = {node: float('inf') for node in graph.nodes()}\n",
    "    \n",
    "    # Define a BFS procedure to calculate distances to {target_node, sink_node}\n",
    "    queue = deque([(instance.target_node, 0), (instance.sink_node, 0)])\n",
    "    while queue:\n",
    "        current_node, current_dist = queue.popleft()\n",
    "        if dist[current_node] == float('inf'):\n",
    "            dist[current_node] = current_dist\n",
    "            if current_dist not in layers:\n",
    "                layers[current_dist] = [current_node]\n",
    "            else:\n",
    "                layers[current_dist].append(current_node)\n",
    "            for neighbor in graph.predecessors(current_node):\n",
    "                if dist[neighbor] == float('inf'):\n",
    "                    queue.append((neighbor, current_dist + 1))\n",
    "    \n",
    "    # Calculate max_dist\n",
    "    print(dist)\n",
    "    max_dist = max(dist.values())\n",
    "    \n",
    "    return layers, max_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phi_set(instance: Arrival, phi: float) :\n",
    "    layers, max_dist = decompose_into_layers(instance) \n",
    "    print(max_dist)\n",
    "    S = []\n",
    "    U = layers[0]\n",
    "    for i in range(1,max_dist+1):\n",
    "        if len(layers[i]) < phi*len(U):\n",
    "            S += layers[i]\n",
    "            U = []\n",
    "        U += layers[i]\n",
    "    return S\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_w(w,w_new):\n",
    "    # if w is None:\n",
    "    #     return False\n",
    "    for key in w:\n",
    "        if w[key] != w_new[key]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def subexponential(instance: Arrival, phi: float):\n",
    "    S = compute_phi_set(instance, phi)\n",
    "    print(\"S is \",S)\n",
    "    w = {s:10 for s in S}\n",
    "    t_d,t_sink, w_new = multi_run_procedure(instance, S, w)\n",
    "    print(w_new)\n",
    "    counter = 1\n",
    "    while not compare_w(w,w_new):\n",
    "        w = w_new\n",
    "        t_d, t_sink, w_new = multi_run_procedure(instance, S, w)\n",
    "        counter += 1\n",
    "        \n",
    "    print(\"t_d is \",t_d, \"t_sink is \",t_sink, \"w is \",w_new)\n",
    "        \n",
    "    return t_d, t_sink, w_new, counter\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating The Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_procedure(instance: Arrival):\n",
    "        v = instance.start_node\n",
    "        s_curr = np.copy(instance.s_0) # current switches for each node\n",
    "        s_next = np.copy(instance.s_1) # next switch for each node\n",
    "        counter = 0\n",
    "        while counter < instance.n * 2**instance.n:\n",
    "            # if counter % 10**8 == 0:\n",
    "            # print(f'move {counter} :{v}')\n",
    "            \n",
    "            if v == -1:\n",
    "                return True\n",
    "            elif v == instance.target_node:\n",
    "                return True\n",
    "            \n",
    "            w = s_curr[v]\n",
    "            s_curr[v] = s_next[v]\n",
    "            s_next[v] = w  \n",
    "            v = w\n",
    "            \n",
    "            counter += 1\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Normal random instance\n",
    "# instances = []\n",
    "# def run_random_instance():\n",
    "#     number_of_nodes = 30\n",
    "#     phi = np.sqrt(3) / np.sqrt(2*number_of_nodes)\n",
    "#     # branch_instace = get_branch_instance(number_of_nodes,split_ratio=0.5)\n",
    "#     for i in range(10):\n",
    "#         print(\"---------------------------------\\n\",\"Instance \",i+1)\n",
    "#         instance = Arrival(number_of_nodes, True)\n",
    "#         instances.append(instance)\n",
    "        \n",
    "#         out = subexponential(instance, phi)\n",
    "#         if out[0] > 0 and out[1] == 0:\n",
    "#             if not run_procedure(instance):\n",
    "#                 return instance\n",
    "#     return None\n",
    "            \n",
    "# instance = run_random_instance()\n",
    "# instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## BRANCH Instances \n",
    "# number_of_nodes = 30\n",
    "# weird_instances = []\n",
    "# non_terminal_instances = []\n",
    "# instances = []\n",
    "# phi = np.sqrt(3) / np.sqrt(2*number_of_nodes)\n",
    "# for i in range(20):    \n",
    "#     r = np.random.uniform(0.1, 0.9)\n",
    "#     instance = get_branch_instance(number_of_nodes, split_ratio=r)\n",
    "#     instances.append(instance)\n",
    "    \n",
    "#     out = subexponential(instance, phi)\n",
    "#     if out[0] == 0 and out[1] == 0:\n",
    "#         weird_instances.append(i)\n",
    "#     elif out[0] == 0 and out[1] != 0:\n",
    "#         non_terminal_instances.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Branch without random \n",
    "# number_of_nodes = 30\n",
    "# weird_instances = []\n",
    "# non_terminal_instances = []\n",
    "# instances = []\n",
    "# phi = np.sqrt(3) / np.sqrt(2*number_of_nodes)\n",
    "# for i in range(1):\n",
    "#         print(\"---------------------------------\\n\",\"Instance \",i+1)\n",
    "#         instance = get_branch_instance_without_random(number_of_nodes, split_ratio=0.4)\n",
    "#         instances.append(instance)\n",
    "        \n",
    "#         out = subexponential(instance, phi)\n",
    "#         if out[0] > 0:\n",
    "#             if not run_procedure(instance):\n",
    "#                 break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarski",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
